\documentclass{beamer}
%
% Choose how your presentation looks.
%
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{
  \usetheme{default}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{default} % or try albatross, beaver, crane, ...
  \usefonttheme{default}  % or try serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
} 

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{comment}

\setbeamerfont{page number in head/foot}{size=\scriptsize}
\setbeamertemplate{footline}[frame number]
\let\otp\titlepage
\renewcommand{\titlepage}{\otp\addtocounter{framenumber}{-1}}

\title{Deep Learning application \\ to large-scale image retrieval }
\author{Pavel Nesterov}
\institute{\url{http://pavelnesterov.info/}}
\date{\today}

\begin{document}

\begin{frame}[plain]
  \titlepage
\end{frame}


% Uncomment these lines for an automatically generated outline.
%\begin{frame}{Outline}
%  \tableofcontents
%\end{frame}

\section{Introduction}


\begin{frame}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{images/stat.png}
\end{figure}

\end{frame}


\begin{frame}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{images/nn.png}
\end{figure}

\end{frame}


\begin{frame}{Linear models}

\begin{columns}[t]
\column{0.5\textwidth}

\begin{block}{Linear regression}
\begin{eqnarray*}
\hat{y} &=& w_0 + \sum_{i=1}^N w_i x_i
\end{eqnarray*}
\end{block}	
	
\column{0.5\textwidth}

\begin{block}{Logistic regression}

\begin{eqnarray*}
\hat{p}\left(y = 1\right) &=& \sigma\left(w_0 + \sum_{i=1}^N w_i x_i\right) \\
\sigma\left(x\right) &=& \frac{1}{1 + e^{-x}}
\end{eqnarray*}
\end{block}
	
\end{columns}

\end{frame}


\begin{frame}{Biological neuron}

\begin{columns}[t]
\column{0.6\textwidth}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.95\textwidth]{images/bneuron.jpg}
\end{figure}
	
\column{0.4\textwidth}

\begin{itemize}
\item \textit{what is common between linear/logistic regression and biological neuron?}
\end{itemize}
	
\end{columns}

\end{frame}


\begin{frame}{Artificial neuron}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.85\textwidth]{images/neuron_model.jpeg}
\end{figure}

\begin{block}{Generalized linear model}
\begin{eqnarray*}
\hat{y} &=& g^{-1}\left(w_0 + \sum_{i=1}^N w_i x_i\right)
\end{eqnarray*}
\end{block}
	

\end{frame}


\begin{frame}{Single layer network}

\begin{columns}[t]
\column{0.5\textwidth}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{images/single_layer.png}
\end{figure}	

\column{0.5\textwidth}

\begin{block}{MaxEnt model}
\begin{eqnarray*}
\hat{h}_k &=& w_{k, 0} + \sum_{i=1}^N w_{k, i} x_i \\
\hat{p}\left(y = C_k\right) &=& \frac{e^{h_k}}{\sum_{j=1}^K e^{h_j}}
\end{eqnarray*}
\end{block}
	
\end{columns}

\end{frame}


\begin{frame}{Universal approximation theorem, \#1}

For any function $f(x)$\footnote{actually not for any, but any function can be decomposed into several "good" ones} we can build $F(x)$, such that it approximates $f(x)$ with any given precision:

\begin{eqnarray*}
F\left(x\right) = \sum_{i=1}^N v_i \phi\left(b_i + \sum_{j=1}^m w_{ij} x_j\right)
\end{eqnarray*}

where $\phi(x)$ is nonconstant and monotonically-increasing continuous function.

\begin{itemize}
\item \textit{which network topology is produced by this theorem?}
\end{itemize}

\end{frame}


\begin{frame}{Universal approximation theorem, \#2}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\textwidth]{images/uat.png}
\end{figure}	

\begin{itemize}
\item \textit{what problem do you see in this theorem?}
\end{itemize}

\end{frame}


\begin{frame}{Shallow fully connected network}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{images/mlp.png}
\end{figure}
 
\begin{itemize}
\item \textit{how many parameters second hidden layer requires?}
\end{itemize}

\end{frame}


\begin{frame}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{images/inception.png}
\end{figure}

\begin{itemize}
\item solution is to simplify network
\end{itemize}

\end{frame}


\begin{frame}{Convolutions}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/kernel_convolution.jpg}
\end{figure}

\begin{itemize}
\item \url{https://github.com/vdumoulin/conv_arithmetic}
\item \textit{nothing is free, what cost did we pay for such trick?}
\end{itemize}

\end{frame}


\begin{frame}{Filter bank}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\textwidth]{images/cnn_explained.png}
\end{figure}

\begin{itemize}
\item \textit{how many parameters second convolutional hidden layer requires now?}
\end{itemize}

\end{frame}


\begin{frame}{Deep convolutional network}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{images/deep_conv.png}
\end{figure}

\end{frame}


\begin{frame}{Very deep convolutional network}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{images/comp1.png}
\end{figure}

\end{frame}


\begin{frame}{Very very deep convolutional network}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{images/comp2.png}
\end{figure}

\end{frame}


\begin{frame}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{images/moar.png}
\end{figure}

\end{frame}


\begin{frame}{State-of-the-art}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{images/rev_depth.png}
\end{figure}

\end{frame}


\begin{frame}{Opening the black box}

Given a network $F$ with parameters $\Theta$, cost function $J$, input image $x$ and its class $c$; lets also define output of each neuron $i$ of layer $j$ as $F_{ij}$:
\begin{columns}[t]
\column{0.5\textwidth}
\begin{block}{Training}
\begin{eqnarray*}
\Delta\Theta = -\lambda \frac{\partial J\left(c, F\left(x, \Theta \right)\right)}{\partial \Theta}
\end{eqnarray*}
\end{block}
	
\column{0.5\textwidth}
\begin{block}{Saliency map}
\begin{eqnarray*}
\Delta x = \frac{\partial F_{ij}\left(x, \Theta \right)}{\partial x}
\end{eqnarray*}
\end{block}

\begin{itemize}
\item \textit{what does $\Delta x$ means?}
\end{itemize}
	
\end{columns}

\end{frame}


\begin{frame}{How network see the world, \#1}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{images/features1.png}
  \caption{Visualizing and Understanding Convolutional Network \footnote{Matthew D. Zeiler and Rob Fergus}}
\end{figure}

\end{frame}


\begin{frame}{How network see the world, \#2}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{images/features2.png}
\end{figure}

\end{frame}


\begin{frame}{How network see the world, \#3}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{images/features3.png}
\end{figure}

\end{frame}


\begin{frame}{Iris filter}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\textwidth]{images/eye1.png}
\end{figure}

\end{frame}


\begin{frame}{Eye filter}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\textwidth]{images/eye2.png}
\end{figure}

\end{frame}


\begin{frame}{Brain}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{images/brain.png}
\end{figure}

\end{frame}


\begin{frame}{Search}

\begin{itemize}
\item shallow layers catch low-level abstract/texture features (usually it doesn't depend on dataset and on cost function)
\item deeper layers catch high-level semantic features (more specific to dataset and to cost function)
\item \textit{can we use it for image search, given this knowledge?}
\end{itemize}

\end{frame}

\begin{comment}
\begin{frame}{Semantic hashing}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\textwidth]{images/sh_schema.png}
  \caption{Semantic representation\footnote{Semantic Hashing (Salakhutdinov, Hinton)}}
\end{figure}

\end{frame}


\begin{frame}{Deep sparse autoencoder}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{images/dae.png}
  \caption{Training of deep autoencoder\footnote{Semantic Hashing (Salakhutdinov, Hinton)}}
\end{figure}

\end{frame}


\begin{frame}{Compression}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{images/vgg_transfer_sh.png}
\end{figure}

\end{frame}

\end{comment}


\begin{frame}{Visualization, \#1}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{images/search1.png}
\end{figure}

\end{frame}


\begin{frame}{Visualization, \#2}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{images/search2.png}
\end{figure}

\end{frame}


\begin{frame}{Visualization, \#3}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{images/search3.png}
\end{figure}

\end{frame}


\begin{frame}{Visualization, \#4}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{images/search4.png}
\end{figure}

\end{frame}


\begin{frame}{Visualization, \#5}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{images/search5.png}
\end{figure}

\end{frame}


\begin{frame}{Visualization, \#6}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{images/search6.png}
\end{figure}

\end{frame}


\begin{frame}{Visualization, \#7}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{images/search7.png}
\end{figure}

\end{frame}


\begin{frame}{Visualization, \#8}

\begin{itemize}
\item segmentation
\item search + segmentation  + style loss
\end{itemize}

\end{frame}


\begin{frame}{Feel free to ask questions}

Play with Artisto
\begin{itemize}
\item \url{https://play.google.com/store/apps/details?id=com.smaper.artisto}
\item \url{https://itunes.apple.com/us/app/artisto-video-photo-editor/id1137893020}
\item \url{https://www.youtube.com/watch?v=VvEqW04vocM}
\item \url{https://www.youtube.com/watch?v=wOjZDo1NXaA}
\end{itemize}

Try new Kaggle computer vision competition
\begin{itemize}
\item \url{https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring}
\end{itemize}

\end{frame}




\begin{comment}
\begin{itemize}
  \item Your introduction goes here!
  \item Use \texttt{itemize} to organize your main points.
\end{itemize}
\vskip 1cm
\begin{block}{Examples}
Some examples of commonly used commands and features are included, to help you get started.
\end{block}
\end{comment}


\end{document}
